# visualizing-neural-mappings

本リポジトリは、**学習前（ランダム初期化）のニューラルネットワークが定義している写像（関数）**を、  
2次元平面上で直感的に可視化するための実験コードです。

深層学習が行っている処理を「重み」や「数式」としてではなく、  
**入力空間がどのように変形・歪曲されるか**という幾何学的観点から理解することを目的としています。

---

## モチベーション

ニューラルネットワークは学習前であっても、すでにある関数  
\[
f: \mathbb{R}^2 \rightarrow \mathbb{R}^2
\]
を定義しています。

本実験では、

- 学習を一切行わず
- ランダム初期化された重みのみを用いて
- 「ネットワーク構造そのもの」が生む写像

を可視化します。

これにより、

- 深さや非線形性が空間に与える影響  
- 「学習とは何をしているのか」という直感的理解  

を促すことを狙っています。

---

## 実験概要

### 1. 入力空間

- 2次元平面上に一様なグリッドを配置します。
- 各点は座標 `(x, y)` を持ちます。
- 可視化のため、左上から右下へ連続的に変化するグラデーション色を各点に割り当てます。
- **色は可視化専用であり、モデルへの入力には使用しません。**

### 2. モデル

- ランダム初期化された多層パーセプトロン（MLP）を使用します。
- 学習は行いません。
- 各層は 2 次元 → 2 次元の写像とします。
- 活性化関数（例：ReLU, tanh）を切り替えて比較可能です。

### 3. 可視化方法

1. 入力点 `(x, y)` をモデルに入力
2. 出力として得られた `(x', y')` を平面上にプロット
3. **入力点に割り当てた色を、そのまま出力点にも付与**

これにより、

- 入力空間のある点が
- 出力空間のどこへ写像されたのか

を色の対応関係として直感的に追跡できます。

### 4. 層を重ねた観察

- 1層目、2層目、… と段階的に可視化します。
- 層が深くなるにつれて、
  - 局所的な引き伸ばし
  - 折れ曲がり
  - 領域の分断
  がどのように現れるかを観察します。
- 乱数シードを変えることで、写像の多様性も確認します。

---

## この実験で分かること

- 学習していなくても、ニューラルネットワークはすでに複雑な関数を持っていること
- 深さと非線形性が、入力空間を強く歪めること
- 学習とは「ランダムな写像を、目的に沿って整形していく過程」と捉えられること

